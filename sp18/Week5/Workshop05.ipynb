{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 5: Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview: generating random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss simulations in greater detail later in the semester. The first step in similating nature -- which, despite Einstein's objections, is playing dice after all -- is to learn how to generate some numbers that appear random. Of course, computers cannot generate true random numbers -- they have to follow an algorithm. But the algorithm may be based on something that is difficult to predict (e.g. the time of day you are executing this code) and therefore *look* random to a human. Sequences of such numbers are called *pseudo-random*. \n",
    "\n",
    "The random variables you generate will be distributed according to some *Probability Density Function* (PDF). The most common PDF is *flat*: $f(x)=\\frac{1}{b-a}$ for $x\\in [a..b]$. Here is how to get a random number uniformly distributed between $a=0$ and $b=1$ in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard preamble\n",
    "import numpy as np\n",
    "import scipy as sp      \n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# generate one random number between [0,1)\n",
    "x = np.random.rand()\n",
    "print ('x=',x)\n",
    "\n",
    "# generate an array of 10 random numbers between [0,1)\n",
    "array = np.random.rand(10)\n",
    "print (array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate a set of randomly-distributed integer values instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0,1000,10)  \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d distributions\n",
    "\n",
    "## Moments of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's SciPy library contains a set of standard statistical functions. See a few examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of data and compute mean and variance\n",
    "# This creates an array of 100 elements, uniformly-distributed between 100 and 200\n",
    "\n",
    "x = np.random.uniform(low=100,high=200,size=100)\n",
    "\n",
    "print(x[0:10])\n",
    "# make a histogram\n",
    "n, bins, patches = plt.hist(x, 20)\n",
    "\n",
    "# various measures of \"average value\":\n",
    "print('Mean = {0:5.0f}'.format(sp.mean(x)))\n",
    "print( 'Median = {0:5.0f}'.format(sp.median(x)))\n",
    "\n",
    "# measure of the spread\n",
    "print('Standard deviation = {0:5.1f}'.format(np.std(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate 100 random numbers, uniformly distributed between [-$\\pi,\\pi$). \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean and standard deviation (RMS)\n",
    "1. Plot a histogram of sin(x) and cos(x), where x is a uniformly distributed random number between [-pi,pi). Do you understand this distribution ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian distribution\n",
    "\n",
    "You can also generate Gaussian-distributed numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a single random number, gaussian-distributed with mean=0 and sigma=1. This is also called \n",
    "# a standard normal distribution\n",
    "x = np.random.standard_normal()\n",
    "print (x)\n",
    "\n",
    "# generate an array of 10 such numbers\n",
    "a = np.random.standard_normal(size=10)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate $N=100$ random numbers, Gaussian-distributed with $\\mu=0$ and $\\sigma=1$. \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n",
    "1. Compute the median of this distribution\n",
    "1. Compute the of this distribution. *Hint*: you may not be able to use *sp.stats.mode()* function ! (why not ?) To compute the mode properly, you need to use the output of the histogram you produced in part 2.\n",
    "1. Now find the means, standard deviations, and errors on the means for each of the $M=1000$ experiments of $N=100$ measurements each. Plot a histogram of the means. Is it consistent with your calculation of the error on the mean for $N=100$ ? About how many experiments yield a result within $1\\sigma$ of the true mean of 0 ? About how many are within $2\\sigma$ ? Is this what you expected?\n",
    "1. Now repeat question 4 for $N=10,50,1000,10000$. Plot a graph of the RMS of the distribution of the means vs $N$. Is it consistent with your expectations ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential distribution\n",
    "\n",
    "In this part we will repeat the above process, but now using lists of exponentially distributed random numbers. The probability of selecting a random number between $x$ and $x+dx$ is $\\propto e^{-x}dx$. Exponential distributions often appear in lossy systems, e.g. if you plot an amplitude of a damped oscillator as a function of time. Or you may see it when you plot the number of decays of a radioactive isotope as a function of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a single random number, exponentially-distributed with scale=1. \n",
    "x = np.random.exponential()\n",
    "print (x)\n",
    "\n",
    "# generate an array of 10 such numbers\n",
    "a = np.random.exponential(size=10)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "1. What do you expect to be the mean of the distribution? What do you expect to be the standard deviation?\n",
    "1. Generate $N=100$ random numbers, exponentially-distributed with scale=1. \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n",
    "1. Now find the means, standard deviations, and errors on the means for each of the $M=1000$ experiments of $N=100$ measurements each. Plot a histogram of each quantity. Is the RMS of the distribution of the means consistent with your calculation of the error on the mean for $N=100$ ? \n",
    "1. Now repeat question 5 for $N=10,100,1000,10000$. Plot a graph of the RMS of the distribution of the means vs $N$. Is it consistent with your expectations ? This is a demonstration of the *Central Limit Theorem*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial distribution\n",
    "The binomial distribution with parameters $n$ and $p$ is the *discrete* probability distribution of the number of successes in a sequence of $n$ independent yes/no experiments, each of which yields success with probability $p$. A typical example is a distribution of the number of *heads* for $n$ coin flips ($p=0.5$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "# flip one coin. 0 = heads, 1 = tails\n",
    "print (np.random.binomial(1,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "1. Generate $N=10$ random coin flips. \n",
    "1. Plot the outcomes in a histogram (0=heads, 1=tails). \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "The Poisson distribution is a *discrete* probability distribution that expresses the probability of a given number of events $n$ occurring in a fixed interval of time $T$ if these events occur with a known average rate $\\nu/T$ and independently of the time since the last event. The *expectation value* of $n$ is $\\nu$. The variance of $n$ is also $\\nu$, so the standard deviation of $n$ is $\\sigma(n) = \\sqrt{\\nu}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 10 # expected number of events\n",
    "n = np.random.poisson(nu)  # generate a Poisson-distributed number\n",
    "print (n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "1. Generate $N=100$ random numbers, Poisson-distributed with $\\nu=10$. \n",
    "1. Plot them in a histogram. \n",
    "1. Compute mean, standard deviation (RMS), and the error on the mean. Is this what you expected?\n",
    "1. Now repeat question 3 for $\\nu=1,5,100,10000$. Plot a graph of the RMS vs $\\nu$. Is it consistent with your expectations ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d distributions\n",
    "\n",
    "You can create two independent samples of events and plot their distribution as a *scatter* plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.standard_normal(size=1000)\n",
    "y = np.random.standard_normal(size=1000)\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the correlation matrix for two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sp.corrcoef(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although more instructive perhaps is to print the full covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sp.cov(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a cute example of plotting projection histograms together with the scatter plot:\n",
    "(from http://matplotlib.org/examples/pylab_examples/scatter_hist.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "# the random data\n",
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)\n",
    "\n",
    "nullfmt = NullFormatter()         # no labels\n",
    "\n",
    "# definitions for the axes\n",
    "left, width = 0.1, 0.65\n",
    "bottom, height = 0.1, 0.65\n",
    "bottom_h = left_h = left + width + 0.02\n",
    "\n",
    "rect_scatter = [left, bottom, width, height]\n",
    "rect_histx = [left, bottom_h, width, 0.2]\n",
    "rect_histy = [left_h, bottom, 0.2, height]\n",
    "\n",
    "# start with a rectangular Figure\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "axScatter = plt.axes(rect_scatter)\n",
    "axHistx = plt.axes(rect_histx)\n",
    "axHisty = plt.axes(rect_histy)\n",
    "\n",
    "# no labels\n",
    "axHistx.xaxis.set_major_formatter(nullfmt)\n",
    "axHisty.yaxis.set_major_formatter(nullfmt)\n",
    "\n",
    "# the scatter plot:\n",
    "axScatter.scatter(x, y)\n",
    "\n",
    "# now determine nice limits by hand:\n",
    "binwidth = 0.25\n",
    "xymax = np.max([np.max(np.fabs(x)), np.max(np.fabs(y))])\n",
    "lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "axScatter.set_xlim((-lim, lim))\n",
    "axScatter.set_ylim((-lim, lim))\n",
    "\n",
    "bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "axHistx.hist(x, bins=bins)\n",
    "axHisty.hist(y, bins=bins, orientation='horizontal')\n",
    "\n",
    "axHistx.set_xlim(axScatter.get_xlim())\n",
    "axHisty.set_ylim(axScatter.get_ylim())\n",
    "\n",
    "axScatter.set_xlabel('x')\n",
    "axScatter.set_ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create a correlated sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # mean values of two variables\n",
    "mean = [0, 0] \n",
    "\n",
    "#  covariance matrix \n",
    "# Note that the covariance matrix must be positive semidefinite (a.k.a. nonnegative-definite). \n",
    "# Otherwise, the behavior of this method is undefined and backwards compatibility is not guaranteed.\n",
    "cov = [[1, 0.8], [0.8, 1]]  \n",
    "\n",
    "# produce a sample \n",
    "x, y = np.random.multivariate_normal(mean, cov, 1000).T   \n",
    "\n",
    "# plot -- this looks like a streak\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "?plt.errorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Produce a *profile* plot from the previous sample, i.e. subdivide $x$ range into bins (e.g. 60 bins from $x\\in [-3..3]$), for each bin in $x$ compute the mean of $y$ distribution, then plot mean of $y$ and its error vs $x$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fitting\n",
    "\n",
    "Examples of how to do a fit to a graph were given in lecture (see Lecture05.ipynb). Let's practice with the fits a bit more. \n",
    "\n",
    "The simplest technique to describe is least-squares fitting (see lecture notes). Usually you use the least-squares fit if you have a graph (i.e. a set of data points $y_i(x_i)$), you want to describe it in terms of a model $y(x;\\theta)$, where parameters $\\theta$ are unknown. You fit to determine the values of $\\theta$ and (hopefully) their uncertainties.  \n",
    "\n",
    "There are two standard cases where least-squares method is applicable:\n",
    "1. You know errors for each data point $\\sigma_i$ and you know that those errors are Gaussian. In this case, you minimize $\\chi^2=\\sum \\left(\\frac{y_i - y(x_i;\\theta)}{\\sigma_i}\\right)^2$. The value of the $\\chi^2_{\\min}$ can be interpreted as a goodness-of-fit, and the errors on parameters $\\theta$ have probabilistic interpretation\n",
    "1. You know that the errors are Gaussian and are the same for each data point, but you do not know their magnitude. In this case, you would minimize the sum of squares: $\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\theta)\\right)^2$. Then value of $\\mathcal{S}$ can be used to compute the errors $\\sigma_i$ for each data point: $\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$\n",
    "The errors on $\\theta$ have a probabilistic definition, but you lose information about the goodness of fit\n",
    "1. If the errors are not known to be Gaussian, then the least square method is not useful to estimate uncertainties or the goodness of fit. It is also not guaranteed to be unbiased or most efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial $\\chi^2$ fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as fitter\n",
    "\n",
    "\n",
    "# Generate artificial data = straight line with a=0 and b=1\n",
    "# plus some noise.\n",
    "a0 = 0\n",
    "b0 = 1\n",
    "sig = 0.2\n",
    "Npoints = 10\n",
    "\n",
    "xdata = np.arange(0,Npoints,1.)\n",
    "ydata = a0+xdata*b0+sig*np.random.standard_normal(size=Npoints)\n",
    "sigma = np.ones(Npoints)*sig\n",
    "\n",
    "# define a fit model. For this part, we will use a linear function\n",
    "def model(x, a, b):\n",
    "    return a + b*x\n",
    "\n",
    "# You have to supply an initial guess of parameters, and they should be \"close enough\" to the true values, otherwise\n",
    "# the fit may fall into a false minimum\n",
    "par0    = np.array([0.0, 1.0])\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0, sigma, absolute_sigma=True)\n",
    "\n",
    "# par arrays contains the values of parameters. cov is the covariance matrix\n",
    "# decode it now\n",
    "a = par[0]\n",
    "ea = np.sqrt(cov[0,0])\n",
    "print('a={0:6.3f}+/-{1:5.3f}'.format(a,ea))\n",
    "b = par[1]\n",
    "eb = np.sqrt(cov[1,1])\n",
    "print('b={0:6.3f}+/-{1:5.3f}'.format(b,eb))\n",
    "\n",
    "# compute reduced chi2\n",
    "chi_squared = np.sum(((model(xdata, *par)-ydata)/sigma)**2)\n",
    "reduced_chi_squared = (chi_squared)/(len(xdata)-len(par))\n",
    "print ('chi^2 = {0:5.2f}'.format(chi_squared))\n",
    "print ('chi^2/d.f.={0:5.2f}'.format(reduced_chi_squared))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit,par[0],par[1]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do an *unweighted* fit if you do not know the uncertainties for each point. *curve_fit* will minimize \n",
    "$\\mathcal{S} = \\sum \\left(y_i - y(x_i;\\theta)\\right)^2$. You can use it to estimate the uncertainty for each point: \n",
    "$\\sigma_i = \\sqrt{\\mathcal{S}/(N_\\mathrm{data}-N_\\mathrm{parameters})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to supply an initial guess of parameters, and they should be \"close enough\" to the true values, otherwise\n",
    "# the fit may fall into a false minimum\n",
    "par0    = np.array([0.0, 1.0])\n",
    "par, cov = fitter.curve_fit(model, xdata, ydata, par0, sigma, absolute_sigma=False)\n",
    "\n",
    "# par arrays contains the values of parameters. cov is the covariance matrix\n",
    "# decode it now\n",
    "a = par[0]\n",
    "ea = np.sqrt(cov[0,0])\n",
    "print ('a={0:6.3f}+/-{1:5.3f}'.format(a,ea))\n",
    "b = par[1]\n",
    "eb = np.sqrt(cov[1,1])\n",
    "print ('b={0:6.3f}+/-{1:5.3f}'.format(b,eb))\n",
    "\n",
    "# compute the error per point\n",
    "sigCalc = np.sqrt(np.sum(((model(xdata, *par)-ydata))**2)/(len(xdata)-len(par)))\n",
    "print ('Generated error = {0:5.2f}'.format(sig))\n",
    "print ('Computed error = {0:5.2f}'.format(sigCalc))\n",
    "\n",
    "# overlay plot over data\n",
    "plt.errorbar(xdata, ydata, xerr=0, yerr=sigma, fmt='o')\n",
    "plt.xlim(-1,Npoints)\n",
    "xfit = np.linspace(0,Npoints-1.,50)\n",
    "plt.plot(xfit,model(xfit,par[0],par[1]),'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to a distribution\n",
    "\n",
    "scipy.stats package provides parameterizations of many standard probability density functions (PDFs). Each PDF has a *fit()* method, which does an *unbinned maximum likelihood fit* to a set of events, constraining the parameters of the PDF. Here is an example of a fit of a set of events to a Gaussian PDF.\n",
    "Courtesy http://glowingpython.blogspot.com/2012/07/distribution-fitting-with-scipy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate 1000 events from a normal distrubution\n",
    "# with mean 0 and standard deviation 1\n",
    "sample = norm.rvs(loc=0,scale=1,size=1000) \n",
    "\n",
    "par = norm.fit(sample) # fit this sample to a gaussian distribution, determine parameters\n",
    "\n",
    "print ('mean={0:4.2f}'.format(par[0]))\n",
    "print ('sigma={0:4.2f}'.format(par[1]))\n",
    "\n",
    "# now, par[0] and par[1] are the mean and \n",
    "# the standard deviation of the fitted distribution\n",
    "x = np.linspace(-5,5,100)\n",
    "# fitted distribution\n",
    "pdf_fitted = norm.pdf(x,loc=par[0],scale=par[1])\n",
    "# original distribution\n",
    "pdf = norm.pdf(x)\n",
    "\n",
    "plt.title('Normal distribution')\n",
    "plt.plot(x,pdf_fitted,'r-',x,pdf,'b-')\n",
    "plt.hist(sample,normed=1,alpha=.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, *scipy.stats.fit* does not provide an esitimate of the covariance matrix (or even the errors on the parameters). This is a major problem with doing max-likelihood fits using *scipy.stats* package ! (any respectable physicist needs to report errors for his/her measurements !). There are more advanced tools for likelihood fitting that do provide error estimates -- but at this point they are beyond the scope of this class. But here are a few pointers:\n",
    "\n",
    "- PyROOT: https://root.cern.ch/pyroot\n",
    "- KaFE: http://www-ekp.physik.uni-karlsruhe.de/~quast/kafe/html/\n",
    "\n",
    "For a Gaussian distribution, and a few others, max-likelihood estimators have analytic formulae:\n",
    "\n",
    "$\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "\n",
    "$\\hat{\\sigma^2} = \\frac{1}{N}\\sum_{i=1}^N (x_i-\\hat{\\mu})^2$\n",
    "\n",
    "$\\sigma(\\hat{\\mu}) = \\frac{\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(sample)\n",
    "mean = np.mean(sample)\n",
    "sigma = np.sqrt(np.sum((sample-mean)**2)/N)\n",
    "eMean = sigma/np.sqrt(N)\n",
    "eSigma=np.sqrt((np.sum((sample-mean)**4)/N-(N-3)/(N-1)*sigma**4)/N)\n",
    "print ('Max-likelihood estimate of mean={0:4.2f}+/-{1:4.2f}'.format(mean,eMean))\n",
    "print ('Max-likelihood estimate of sigma={0:4.2f}+/-{1:4.2f}'.format(sigma,eSigma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common way to determine parameters of a distribition is to do perform a *binned least-squares fit*, i.e. fit a distribution to a histogram. When doing so, it is important to assign proper Poisson errors to each bin of a histogram. Here is how you can create a histogram with Poisson errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, bin_edges = np.histogram(sample, range=(-4,4),bins=40)\n",
    "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "ey = np.sqrt(y)\n",
    "# have to be careful to make sure errors are never zero, or chi2 fit would fail ! Assign minimum error of 1 to all bins\n",
    "ey = [max(error,1) for error in ey]\n",
    "\n",
    "plt.errorbar(x,y,ey, fmt='o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Fit this histogram to a Gaussian function using a chi2 fit, print mean and sigma parameters and their uncertainties. *Hint*: use *scipy.optimize.curve_fit* example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
